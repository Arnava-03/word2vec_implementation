# -*- coding: utf-8 -*-
"""word2vec_module.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RyW10Vib3KYaPa2FUMn0Gl4oi8CIqHSp

Creating custom word2vec class for further development into a full-fledged api <br>
Continuous skip gram model
"""

import numpy as np
import re
from collections import Counter

class myWord2Vec():
  def __init__(self, embedding_dim=10, window=2, min_count=1, lr=0.05, epochs=50):
    self.embedding_dim = embedding_dim
    self.window = window #for prediction of range (=window) of words
    self.min_count = min_count #minimum word frequency for inclusion in vocabulary
    self.lr = lr
    self.epochs = epochs
    self.word2idx = {}
    self.idx2word = {}
    self.vocab = {} #stores tokens in the form of (word:freq)
    self.vocab_size = 0
    self.tokens = []
    self.model = None

  def process_data(self, corpus):
    #step1 : tokenise
    text = re.sub(r"[^\w\s]", "", corpus.lower())
    tokens = text.split()

    #step2 : maintain vocabulary
    word_count = Counter(tokens)
    self.vocab = {word: count for word, count in word_count.items() if count >= self.min_count}
    self.vocab_size = len(self.vocab)

    #mappings
    self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}
    self.idx2word = {idx: word for idx, word in enumerate(self.vocab)}

    #create final tokens
    self.tokens = [word for word in tokens if word in self.vocab]


  #helper function 1
  def one_hot_encode(self, idx, size):
    one_hot = np.zeros(size)
    one_hot[idx] = 1
    return one_hot

  def generate_training_data(self):
    X = []
    y = []
    for i in range(len(self.tokens)):
      context_start = max(0, i - self.window)
      context_end = min(len(self.tokens), i + self.window + 1)
      context = self.tokens[context_start:i] + self.tokens[i+1:context_end]
      for j in range(len(context)):
        X.append(self.one_hot_encode(self.word2idx[self.tokens[i]], self.vocab_size))
        y.append(self.one_hot_encode(self.word2idx[context[j]], self.vocab_size))
    return np.array(X), np.array(y)

  def initmodel(self):
    self.model = {
        'w1': np.random.randn(self.vocab_size, self.embedding_dim),
        'w2': np.random.randn(self.embedding_dim, self.vocab_size)
    }

  #helper function 2
  def softmax(self, X):
    res = []
    for x in X:
      exp = np.exp(x)
      res.append(exp / exp.sum())
    return np.array(res)

  def forwardpass(self, X, return_cache = True):
    w1, w2 = self.model['w1'], self.model['w2']
    cache = {}
    cache['a1'] = np.dot(X, w1)
    cache['a2'] = np.dot(cache['a1'], w2)
    cache['z'] = self.softmax(cache['a2'])
    if not return_cache:
      return cache['z']

    return cache


  #helper function 3
  def crossentropy(self, y_pred, y):
    return -np.sum(np.log(y_pred)*y)

  def backprop(self, X, y):
    cache = self.forwardpass(X, return_cache=True)
    w1, w2 = self.model['w1'], self.model['w2']
    dz2 = cache['z'] - y
    dw2 = np.dot(cache['a1'].T, dz2)
    dz1 = np.dot(dz2, w2.T)
    dw1 = np.dot(X.T, dz1)
    self.model['w1'] -= self.lr * dw1
    self.model['w2'] -= self.lr * dw2
    return self.crossentropy(cache['z'], y) # returning loss - useful to track progress of training

  def train(self, corpus):
    self.process_data(corpus)
    X, y = self.generate_training_data()
    self.initmodel()
    loss = [self.backprop(X,y) for _ in range(self.epochs)]
    return loss

  def get_embedding(self, word):
    try:
      idx = self.word2idx[word]
    except KeyError:
      print(f'Word {word} not in vocab')
      return None
    return self.model['w1'][idx]

  def save_embeddings(self, file_path):
    np.savez(
      file_path,
      w1=self.model['w1'],
      word2idx=self.word2idx,
      idx2word=self.idx2word,
      embedding_dim=self.embedding_dim,
      window_size=self.window
    )

  def load_embeddings(self, file_path):
      data = np.load(file_path, allow_pickle=True)
      self.model['w1'] = data['w1']
      self.word2idx = data['word2idx'].item()  
      self.idx2word = data['idx2word'].item()  
      self.embedding_dim = data['embedding_dim'].item()  
      self.window = data['window_size'].item()